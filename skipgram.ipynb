{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 1.4170653041108827\n",
      "Epoch 2000, Loss: 1.4124221105552088\n",
      "Epoch 3000, Loss: 1.4099909627401013\n",
      "Epoch 4000, Loss: 1.4083189623870729\n",
      "Epoch 5000, Loss: 1.40707420933212\n",
      "Epoch 6000, Loss: 1.4060949930419062\n",
      "Epoch 7000, Loss: 1.4052916119270766\n",
      "Epoch 8000, Loss: 1.4046114561888654\n",
      "Epoch 9000, Loss: 1.4040219525061388\n",
      "Epoch 10000, Loss: 1.403501841099461\n",
      "Trained V (Input-to-Hidden weights):\n",
      "[[ 0.0501589   2.0982607 ]\n",
      " [ 0.05907812 -1.20903335]\n",
      " [ 0.3604125   2.11632864]]\n",
      "Trained U (Hidden-to-Output weights):\n",
      "[[ 0.39285162  0.24604211  0.36110627]\n",
      " [-1.20106035  3.81588708 -1.21482674]]\n",
      "Output probabilities for context words given 'I': [2.70032579e-05 9.99946804e-01 2.61926733e-05]\n",
      "Probability of 'I' given 'I': 0.000\n",
      "Probability of 'like' given 'I': 1.000\n",
      "Probability of 'mangos' given 'I': 0.000\n",
      "Output probabilities for context words given 'like': [0.49574174 0.00114079 0.50311747]\n",
      "Probability of 'I' given 'like': 0.496\n",
      "Probability of 'like' given 'like': 0.001\n",
      "Probability of 'mangos' given 'like': 0.503\n",
      "Output probabilities for context words given 'mangos': [2.58125918e-05 9.99949401e-01 2.47861937e-05]\n",
      "Probability of 'I' given 'mangos': 0.000\n",
      "Probability of 'like' given 'mangos': 1.000\n",
      "Probability of 'mangos' given 'mangos': 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary and context pairs\n",
    "vocab = [\"I\", \"like\", \"mangos\"]\n",
    "context_pairs = [(\"I\", \"like\"), (\"like\", \"I\"), (\"like\", \"mangos\"), (\"mangos\", \"like\")]\n",
    "\n",
    "# One-hot encoding dictionary\n",
    "one_hot_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_vector(word):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    vec[one_hot_dict[word]] = 1\n",
    "    return vec\n",
    "\n",
    "# Initialize weights\n",
    "# Initialize weights\n",
    "embedding_dim = 2\n",
    "V = np.array([[0.2, 0.4], [0.1, 0.3], [0.5, 0.7]])\n",
    "U = np.array([[0.6, 0.1, 0.3], [0.4, 0.8, 0.2]])\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Forward pass\n",
    "def forward(center_word):\n",
    "    h = np.dot(V.T, one_hot_vector(center_word))\n",
    "    output = np.dot(U.T, h)\n",
    "    y_pred = softmax(output)\n",
    "    return y_pred, h\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.02\n",
    "epochs = 10000\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, target in context_pairs:\n",
    "        y_pred, h = forward(center)\n",
    "        y_true = one_hot_vector(target)\n",
    "\n",
    "        # Calculate loss (cross-entropy)\n",
    "        loss = -np.log(y_pred[one_hot_dict[target]])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        # Gradients\n",
    "        dU = np.outer(h, error)\n",
    "        dV = np.outer(one_hot_vector(center), np.dot(U, error))\n",
    "\n",
    "        # Update weights\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\n",
    "# Output the trained weights\n",
    "print(\"Trained V (Input-to-Hidden weights):\")\n",
    "print(V)\n",
    "print(\"Trained U (Hidden-to-Output weights):\")\n",
    "print(U)\n",
    "\n",
    "# Check the final probabilities for the word \"I\"\n",
    "center_word = \"I\"\n",
    "output_probabilities, _ = forward(center_word)\n",
    "print(f\"Output probabilities for context words given '{center_word}': {output_probabilities}\")\n",
    "\n",
    "# Output the corresponding words with their probabilities\n",
    "for word, prob in zip(vocab, output_probabilities):\n",
    "    print(f\"Probability of '{word}' given '{center_word}': {prob:.3f}\")\n",
    "\n",
    "# Check the final probabilities for the word \"like\"\n",
    "center_word = \"like\"\n",
    "output_probabilities, _ = forward(center_word)\n",
    "print(f\"Output probabilities for context words given '{center_word}': {output_probabilities}\")\n",
    "\n",
    "# Output the corresponding words with their probabilities\n",
    "for word, prob in zip(vocab, output_probabilities):\n",
    "    print(f\"Probability of '{word}' given '{center_word}': {prob:.3f}\")\n",
    "\n",
    "# Check the final probabilities for the word \"mangos\"\n",
    "center_word = \"mangos\"\n",
    "output_probabilities, _ = forward(center_word)\n",
    "print(f\"Output probabilities for context words given '{center_word}': {output_probabilities}\")\n",
    "\n",
    "# Output the corresponding words with their probabilities\n",
    "for word, prob in zip(vocab, output_probabilities):\n",
    "    print(f\"Probability of '{word}' given '{center_word}': {prob:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
